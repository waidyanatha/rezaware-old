{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Online Travel Agent (OTA) Property Prices\n",
    "\n",
    "___Changes in this notebook must be migrated to the ```dags/modules/ota/etlotaPropertyPrices.py```___\n",
    "\n",
    "@Nileka add a description here as to what this notebook offers\n",
    "\n",
    "### Execute the cell below once to turn off debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    WARNING CONTROL to display or ignore all warnings\n",
    "'''\n",
    "import warnings; warnings.simplefilter('ignore')     #switch betweeb 'default' and 'ignore'\n",
    "import traceback\n",
    "\n",
    "''' Set debug flag to view extended error messages; else set it to False to turn off debugging mode '''\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the OTA scrape class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All functional APP-libraries in REZAWARE-package of REZAWARE-module imported successfully!\n",
      "All scraper in ota software packages loaded successfully!\n",
      "All scraper software packages loaded successfully!\n",
      "All functional FILEWORKLOADS-libraries in LOAD-package of ETL-module imported successfully!\n",
      "FileWorkLoads Class initialization complete\n",
      "Initialing scraper class for scraperUtils with instance Utilities class for property data scraping\n",
      "NatLanWorkLoads Class initialization complete\n",
      "FileWorkLoads Class initialization complete\n",
      "FileWorkLoads Class initialization complete\n",
      "Initialing scraper class for scraperUtils with instance Scrape OTA booking prices\n",
      "FileWorkLoads Class initialization complete\n",
      "\n",
      "Class initialization and load complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "sys.path.insert(1,\"/home/nuwan/workspace/rezaware/\")\n",
    "import rezaware as reza\n",
    "from wrangler.modules.ota.scraper import scraperUtils as otasu\n",
    "from wrangler.modules.ota.scraper import propertyScrapers as otaps\n",
    "from utils.modules.etl.load import sparkFILEwls as spark\n",
    "from utils.modules.etl.load.sparkFILEwls import credentials as cred\n",
    "# from utils.modules.etl.load import sparkwls as spark\n",
    "# from utils.modules.ml.natlang import nlp\n",
    "\n",
    "''' restart initiate classes '''\n",
    "if debug:\n",
    "    import importlib\n",
    "    reza = importlib.reload(reza)\n",
    "    otasu = importlib.reload(otasu)\n",
    "    otaps = importlib.reload(otaps)\n",
    "    spark = importlib.reload(spark)\n",
    "#     nlp = importlib.reload(nlp)\n",
    "\n",
    "__desc__ = \"Scrape OTA booking prices\"\n",
    "''' optional - if not specified class will use the default values '''\n",
    "prop_kwargs = {\"WRITETOTMP\":True,   # necessary to emulate the etl dag\n",
    "              }\n",
    "clsScraper = otaps.PropertyScraper(desc=__desc__, **prop_kwargs)\n",
    "clsUtils=otasu.Utils(desc=__desc__)\n",
    "clsFile = spark.FileWorkLoads(desc=__desc__, **prop_kwargs)\n",
    "print(\"\\nClass initialization and load complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the set of OTA URL for Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing booking.com ...\n",
      "23/03/17 14:37:52 WARN Utils: Your hostname, FarmRaiderTester resolves to a loopback address: 127.0.1.1; using 192.168.124.15 instead (on interface enp2s0)\n",
      "23/03/17 14:37:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/03/17 14:37:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/03/17 14:37:54 WARN FileSystem: Cannot load filesystem: java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem Unable to get public no-arg constructor\n",
      "23/03/17 14:37:54 WARN FileSystem: java.lang.NoClassDefFoundError: com/google/api/client/auth/oauth2/Credential\n",
      "23/03/17 14:37:54 WARN FileSystem: java.lang.ClassNotFoundException: com.google.api.client.auth.oauth2.Credential\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameterized URL list saved in scraper-build-scrape-url-list.csv at wrangler/data/ota/scraper/hospitality/bookings/tmp/\n",
      "Completed parameterizing urls with 30 instances.\n"
     ]
    }
   ],
   "source": [
    "# file = \"otaInputURLsCopy1.json\"\n",
    "file = \"otaInputURLs.json\"\n",
    "start_date = date.today()\n",
    "end_date = start_date + timedelta(days=1)\n",
    "\n",
    "try:\n",
    "    if start_date < date.today():\n",
    "        raise ValueError(\"Start date must be greater than today: %s\" % str(date.today()))\n",
    "    if end_date <= start_date:\n",
    "        raise ValueError(\"End date %s is invalid. It must be greater than Start Date: %s\" % (str(end_date),str(start_date)))\n",
    "    urls_kwargs = {\"PAGEOFFSET\" :10,\n",
    "                   \"PAGEUPLIMIT\":10,\n",
    "                   \"FROMDATE\":start_date,\n",
    "                   \"TODATE\" : end_date,\n",
    "                   \"WRITETOTMP\":True,\n",
    "                  }\n",
    "\n",
    "    _otaURLDirPath, _otaURLFileName, _ota_url_parameterized_list = \\\n",
    "                                    clsScraper.build_scrape_url_list(\n",
    "                                                    file_name=file,  # mandatory to give the inputs json file\n",
    "                                                    dir_path=None,   # optional to be used iff required\n",
    "                                                    **urls_kwargs\n",
    "                                                    )\n",
    "    if _otaURLFileName:\n",
    "        print(\"Parameterized URL list saved in %s at %s\" % (_otaURLFileName,_otaURLDirPath))\n",
    "    if len(_ota_url_parameterized_list)>0:\n",
    "        print(\"Completed parameterizing urls with %d instances.\"\n",
    "              % (len(_ota_url_parameterized_list)))\n",
    "\n",
    "except Exception as err:\n",
    "    _s_fn_id = \"Class <WebScraper> Function <read_folder_csv_to_df>\"\n",
    "    print(\"[Error]\"+_s_fn_id, err)\n",
    "    print(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare folder & file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created folder with search datetime to extract scrape data wrangler/data/ota/scraper/hospitality/bookings/search/booking-com-2023-3-17-9-0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "dirPath = None\n",
    "search_kwargs = {\n",
    "    'MININTERVAL': 0,\n",
    "    \"TIMESTAMP\":datetime.now()+timedelta(minutes=1),\n",
    "    \"PREFIX\":'booking-com',\n",
    "}\n",
    "# _search_dt = datetime.now()\n",
    "# _search_dt = _search_dt + (datetime.min - _search_dt) \\\n",
    "#                     % timedelta(minutes=search_kwargs['MININTERVAL'])\n",
    "# ''' include the timezone '''\n",
    "# _search_dt = (_search_dt.replace(tzinfo=timezone.utc)).isoformat()\n",
    "\n",
    "_current_search_data_store_dir = clsScraper.make_storage_dir(**search_kwargs)\n",
    "print(\"Created folder with search datetime to extract scrape data %s\" \n",
    "      % (_current_search_data_store_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape data into CSVs\n",
    "___TODO:___ hault if internet connection times out; might be possible with airflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrangler/data/ota/scraper/hospitality/bookings/tmp/\n",
      "[Error]function <_scrape_bookings_to_csv> No data received for https://www.booking.com/searchresults.en-gb.html?ss=Las+Vegas&label=gen173nr-1DCAEoggI46AdIM1gEaIUBiAEBmAEJuAEXyAEM2AED6AEBiAIBqAIDuAK_g7aYBsACAdICJDFiNWFiNzM3LTQ0YmItNDIzOC04NDM0LWRjMDFlNmZhYTUwM9gCBOACAQ&sid=36e124997ccdbec4823b6c98d5931c2e&aid=304142&lang=en-gb&sb=1&src_elem=sb&src=searchresults&dest_id=20030916&dest_type=city&checkin=2023-03-17&checkout=2023-03-18&group_adults=1&no_rooms=1&group_children=0&selected_currency=USD&offset=0\n",
      "[Error]function <_scrape_bookings_to_csv> No data received for https://www.booking.com/searchresults.en-gb.html?ss=Las+Vegas&label=gen173nr-1DCAEoggI46AdIM1gEaIUBiAEBmAEJuAEXyAEM2AED6AEBiAIBqAIDuAK_g7aYBsACAdICJDFiNWFiNzM3LTQ0YmItNDIzOC04NDM0LWRjMDFlNmZhYTUwM9gCBOACAQ&sid=36e124997ccdbec4823b6c98d5931c2e&aid=304142&lang=en-gb&sb=1&src_elem=sb&src=searchresults&dest_id=20014181&dest_type=city&checkin=2023-03-17&checkout=2023-03-18&group_adults=1&no_rooms=1&group_children=0&selected_currency=USD&offset=0\n",
      "Scraping completed 30 data files saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "_search_dt=datetime.now()\n",
    "kwargs={\n",
    "    \"header\":'true'\n",
    "}\n",
    "\n",
    "_l_saved_files = clsScraper.scrape_url_list(\n",
    "    ota_url_file_name=_otaURLFileName,\n",
    "    ota_url_dir_path =_otaURLDirPath,\n",
    "    search_datetime = None, #_search_dt,\n",
    "    save_in_dir =None,#_current_search_data_store_dir,\n",
    "    **kwargs,\n",
    ")\n",
    "\n",
    "if len(_l_saved_files)>0:\n",
    "    print(\"Scraping completed %d data files saved!\" \n",
    "          % (len(_l_saved_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV into spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsFile.storeRoot=\"/home/nuwan/workspace/rezaware/\"\n",
    "_folder_path='wrangler/data/ota/scraper/hospitality/bookings/search/2023-3-17-14-0'\n",
    "_fname='booking.com.20014181.2023-03-17.010.csv'\n",
    "kwargs={\n",
    "    \"header\":'true',\n",
    "    \"delimiter\":',',\n",
    "    \"inferSchema\":'false',\n",
    "}\n",
    "scraped_sdf = clsFile.read_files_to_dtype(\n",
    "    as_type='spark',\n",
    "    folder_path=_folder_path,\n",
    "    file_name=_fname,\n",
    "    file_type='csv',\n",
    "    **kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------\n",
      " ota_name          | booking.com          \n",
      " search_dt         | 2023-03-17T14:38:... \n",
      " checkin_date      | 2023-03-17           \n",
      " destination_id    | 20014181             \n",
      " property_name     | Best Western Airp... \n",
      " room_type         | King Room            \n",
      " room_rate         | null                 \n",
      " review_score      | 6.9                  \n",
      " location_desc     | Los Angeles          \n",
      " distance_desc     | Los Angeles          \n",
      " room_desc         | 1 extra-large dou... \n",
      " breakfast         | Breakfast included   \n",
      " cancellations     | null                 \n",
      " availability      | null                 \n",
      " _star_rating_info | null                 \n",
      " star_rating       | null                 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "scraped_sdf.filter(scraped_sdf.breakfast.isNotNull()).show(n=1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the data\n",
    "augment the dataframe with: \n",
    "1. exctract the room rate decimal numbers\n",
    "1. matching city names to the codes \n",
    "1. categorizing the room types based on the taxonomy\n",
    "1. setting the data types of the columns\n",
    "using a transform function in the properties class\n",
    "\n",
    "### Extract room rate decimal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' define room price column to extrac number'''\n",
    "rate_col_name = \"room_rate\"\n",
    "aug_col_name = \"room_price\"\n",
    "''' extract the price value from room rate'''\n",
    "_search_sdf = clsScraper.extract_room_rate(_search_sdf,rate_col_name,aug_col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorize room type by similarity mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait a moment this may take a while categorizing 3018 room type\n",
      "Assigned 59 room categories\n"
     ]
    }
   ],
   "source": [
    "_save_rcate_to = os.path.join(DATA_DIR,'tmp/similarity_categorized_rooms.csv')\n",
    "''' categorize the room types '''\n",
    "emb_kwargs = {\n",
    "    'LOWER':True,\n",
    "    'NO_STOP_WORDS':False,\n",
    "    'METRIC':\"COSIN\",\n",
    "    'MAX_SCORES':2,\n",
    "    'TOLERANCE':0.7,\n",
    "    'ROOM_CATE_FNAME':\"room_descriptions.csv\",\n",
    "}\n",
    "print(\"wait a moment this may take a while categorizing %d room type\" % (_search_sdf.shape[0]))\n",
    "_categorized_room_df = clsScraper.merge_similar_room_cate(_search_sdf,emb_kwargs)\n",
    "_room_cate_count = len(_categorized_room_df.room_cate.unique())\n",
    "print(\"Assigned %d room categories\" % _room_cate_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign location city names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged 93606 rows with destination name and type\n"
     ]
    }
   ],
   "source": [
    "_aug_dest_df = clsScraper.assign_lx_name(data_df=_categorized_room_df)\n",
    "print(\"merged %d rows with destination name and type\" % _aug_dest_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cleaned SDF to Tmp File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/24 19:23:10 WARN TaskSetManager: Stage 10 contains a task of very large size (5187 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/24 19:23:12 WARN TaskSetManager: Stage 13 contains a task of very large size (5187 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/24 19:23:14 WARN TaskSetManager: Stage 16 contains a task of very large size (5187 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/24 19:23:18 WARN TaskSetManager: Stage 17 contains a task of very large size (5187 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 17:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed data saved to /home/nuwan/workspace/rezaware/utils/data/etl/load/tmp/load_save_sdf_to.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "''' convert dataframe from panas to spark '''\n",
    "_tmp_fname = clsSparkWL.save_sdf_to_csv(_aug_dest_df)\n",
    "print(\"transformed data saved to %s\" % _tmp_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV from Tmp and Save to DB Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "22/10/24 19:40:05 WARN TaskSetManager: Stage 41 contains a task of very large size (5510 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait a moment while we insert data int ota_property_prices\n",
      "22/10/24 19:40:07 WARN TaskSetManager: Stage 44 contains a task of very large size (5510 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to ota_property_prices complete!\n",
      "22/10/24 19:40:16 WARN TaskSetManager: Stage 45 contains a task of very large size (5510 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 45:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93606 Data saved to ota_property_prices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "''' Save dataframe to table '''\n",
    "_s_tbl_name = \"ota_property_prices\"\n",
    "_get_tmp_sdfm,traceback = clsSparkWL.read_csv_to_sdf(filesPath=_tmp_fname)\n",
    "# _get_tmp_sdf.printSchema()\n",
    "count, saved_df = clsScraper.save_to_db(data_df=_aug_dest_df,table_name = _s_tbl_name)\n",
    "# count = clsSparkWL.insert_sdf_into_table(save_sdf=_get_tmp_sdf, dbTable=_s_tbl_name)\n",
    "print(\"%d Data saved to %s\" % (count,_s_tbl_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(_get_tmp_sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait a moment, retrieving data ...\n",
      "Loading complete!\n",
      "175944 records loaded from ota_property_prices\n"
     ]
    }
   ],
   "source": [
    "_s_tbl_name = \"ota_property_prices\"\n",
    "data = clsSparkWL.get_data_from_table(dbTable=_s_tbl_name)\n",
    "print(\"%d records loaded from %s\" %(data.count(),_s_tbl_name))\n",
    "\n",
    "# data.select(\"*\").distinct().where(date('created_dt') >= date.today()).sort(\"search_dt\").show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- destination_name: string (nullable = true)\n",
      " |-- destination_id: integer (nullable = true)\n",
      "\n",
      "-RECORD 0-------------------------\n",
      " destination_name | Las Vegas     \n",
      " destination_id   | 20079110      \n",
      "-RECORD 1-------------------------\n",
      " destination_name | New York City \n",
      " destination_id   | 20088325      \n",
      "only showing top 2 rows\n",
      "\n",
      "Destination dictionary loarded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2095 destination names augmented to dataframe!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " search_dt        | 2022-09-24 02:45:00                                                                                                                                                                                                                                     \n",
      " checkin_date     | 2022-09-23 00:00:00                                                                                                                                                                                                                                     \n",
      " property_name    | Extended Stay America Suites - Orlando - Lake Buena Vista                                                                                                                                                                                               \n",
      " room_type        | Deluxe King Studio - Non-Smoking                                                                                                                                                                                                                        \n",
      " room_rate        | 90.0                                                                                                                                                                                                                                                    \n",
      " review_score     | 7.2                                                                                                                                                                                                                                                     \n",
      " location_desc    | Lake Buena Vista, OrlandoShow on map12.1 miles from centre                                                                                                                                                                                              \n",
      " other_info       | Deluxe King Studio - Non-SmokingPrivate studio • 1 bathroom • 301.4feet²1 extra-large double bedBreakfast includedFREE cancellation • No prepayment neededYou can cancel later, so lock in this great price today.Only 1 left at this price on our site \n",
      " currency         | US$                                                                                                                                                                                                                                                     \n",
      " destination_name | Orlando                                                                                                                                                                                                                                                 \n",
      " destination_id   | 20023488                                                                                                                                                                                                                                                \n",
      "-RECORD 1-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " search_dt        | 2022-09-24 02:45:00                                                                                                                                                                                                                                     \n",
      " checkin_date     | 2022-09-23 00:00:00                                                                                                                                                                                                                                     \n",
      " property_name    | Holiday Inn Express Orlando-Ocoee East, an IHG Hotel                                                                                                                                                                                                    \n",
      " room_type        | Queen Room with Two Queen Beds - Non-Smoking                                                                                                                                                                                                            \n",
      " room_rate        | 190.0                                                                                                                                                                                                                                                   \n",
      " review_score     | 7.9                                                                                                                                                                                                                                                     \n",
      " location_desc    | OrlandoShow on map6.2 miles from centre                                                                                                                                                                                                                 \n",
      " other_info       | Queen Room with Two Queen Beds - Non-Smoking2 large double bedsBreakfast includedOnly 6 rooms left at this price on our site                                                                                                                            \n",
      " currency         | US$                                                                                                                                                                                                                                                     \n",
      " destination_name | Orlando                                                                                                                                                                                                                                                 \n",
      " destination_id   | 20023488                                                                                                                                                                                                                                                \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' DEPRECATED -- moved as a function in otaPropertyScraper class '''\n",
    "\n",
    "''' Get destination id dictionary '''\n",
    "destDirPath = os.path.join(DATA_PATH, 'destinations/')\n",
    "destinations_sdf = clsSparkWL.read_csv_to_sdf(filesPath=destDirPath)\n",
    "destinations_sdf = destinations_sdf.selectExpr(\"city as destination_name\", \\\n",
    "                                                \"destinationID as destination_id\")\n",
    "# destinations_sdf = destinations_sdf.withColumn(\"destination_name\",col(\"destination_name\").cast(StringType())) \\\n",
    "#                                 .withColumn(\"destination_id\",col(\"destination_id\").cast(StringType()))\n",
    "if debug:\n",
    "    destinations_sdf.printSchema()\n",
    "    destinations_sdf.show(n=2, vertical=True, truncate=False)\n",
    "    print(\"Destination dictionary loarded!\")\n",
    "\n",
    "''' Lookup & augment destination name '''\n",
    "#aug_search_sdf = destinations_sdf.join(_search_sdf,on='destination_id',how='rightouter')\n",
    "aug_search_sdf = _search_sdf.join(destinations_sdf,\n",
    "                                  _search_sdf.destination_id == destinations_sdf.destination_id,\n",
    "                                  how='leftouter').drop(_search_sdf.destination_id)\n",
    "\n",
    "if debug:\n",
    "    print(\"%d destination names augmented to dataframe!\" % (aug_search_sdf.count()))\n",
    "    aug_search_sdf.show(n=2, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'printSchema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# _search_sdf=_search_sdf.withColumn(\"currency\", lit(\"US$\"))\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# _search_sdf=_search_sdf.withColumn('room_rate', substring('room_rate', 4,10))\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     .withColumn(\"room_rate\",col(\"room_rate\").cast(FloatType()))\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# #    .withColumn(\"search_datetime\",col(\"search_datetime\").cast(DateType()))\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m debug:\n\u001b[0;32m---> 15\u001b[0m     \u001b[43m_search_sdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprintSchema\u001b[49m()\n\u001b[1;32m     16\u001b[0m     _search_sdf\u001b[38;5;241m.\u001b[39mshow(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, vertical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSplit and Extraction complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5569\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5570\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5571\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5573\u001b[0m ):\n\u001b[1;32m   5574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'printSchema'"
     ]
    }
   ],
   "source": [
    "''' DEPRECATED - replaced with a function on otaPropertyScrpaer calss '''\n",
    "\n",
    "from pyspark.sql.functions import substring,lit,col\n",
    "from pyspark.sql.types import StringType,BooleanType,DateType,DecimalType,FloatType, IntegerType,LongType, ShortType, TimestampType\n",
    "\n",
    "# _search_sdf=_search_sdf.withColumn(\"currency\", lit(\"US$\"))\n",
    "# _search_sdf=_search_sdf.withColumn('room_rate', substring('room_rate', 4,10))\n",
    "\n",
    "# ''' reset data types to match table '''\n",
    "# _search_sdf = _search_sdf.withColumn(\"destination_id\",col(\"destination_id\").cast(StringType())) \\\n",
    "#     .withColumn(\"room_rate\",col(\"room_rate\").cast(FloatType()))\n",
    "# #    .withColumn(\"search_datetime\",col(\"search_datetime\").cast(DateType()))\n",
    "\n",
    "if debug:\n",
    "    _search_sdf.printSchema()\n",
    "    _search_sdf.show(n=2, vertical=True, truncate=False)\n",
    "print(\"Split and Extraction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to None\n",
      "Room type categorization complete!\n"
     ]
    }
   ],
   "source": [
    "''' DEPRECATED -- merge moved to the function in otaPropScraper '''\n",
    "\n",
    "_categorized_room_df = _search_sdf.merge(_room_type_assign, how='left', left_on=['room_type'], right_on=['room_type'])\n",
    "print(\"Merged %d rows with categorized room type information\" % _categorized_room_df.shape[0])\n",
    "\n",
    "_categorized_room_df = new_df\n",
    "#_categorized_room_df.to_csv(_save_rcate_to)\n",
    "#_save_rcate_to=clsSparkWL.save_sdf_to_csv(_save_rcate_to)\n",
    "print(\"Merged data saved to %s\" % (_save_rcate_to))\n",
    "print(\"Room type categorization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' DEPRECATED '''\n",
    "_categorized_room_df = _search_sdf.merge(_room_type_assign, how='left', left_on=['room_type'], right_on=['room_type'])\n",
    "_categorized_room_df.to_csv(os.path.join(ROOT_DIR,'data/tmp/similarity_scores.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error]Class <SparkWorkLoads> Function <read_folder_csv_to_sdf> Unable to infer schema for CSV. It must be specified manually.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nuwan/workspace/rezaware/utils/modules/etl/load/sparkwls.py\", line 425, in read_csv_to_sdf\n",
      "    _csv_to_sdf = self.spark_session.read.options( \\\n",
      "  File \"/opt/spark/python/pyspark/sql/readwriter.py\", line 535, in csv\n",
      "    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/spark/python/pyspark/sql/utils.py\", line 196, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: Unable to infer schema for CSV. It must be specified manually.\n",
      "\n",
      "No data loaded by spark; process failed!\n"
     ]
    }
   ],
   "source": [
    "''' DEPRECATED using sparkFILEwls '''\n",
    "# DATA_DIR=\"/home/nuwan/workspace/rezaware/wrangler/data/ota/scraper/hospitality/bookings/\"\n",
    "spark_kwargs = {\"TO_PANDAS\":True,   # change spark dataframe to pandas\n",
    "                \"IS_FOLDER\":True,   # if folder then check if folder is empty\n",
    "                \"INFERSCHEMA\":False # set inferSchema to True or False\n",
    "               }\n",
    "# _current_search_data_store_dir = os.path.join(DATA_DIR, \"rates/2022-10-5-3-0/\")\n",
    "_search_sdf, traceback = clsSparkWL.read_csv_to_sdf(\n",
    "    filesPath=_current_search_data_store_dir,\n",
    "    **spark_kwargs)\n",
    "if not traceback:\n",
    "    print(\"Spark loaded %d rows\" % _search_sdf.shape[0])\n",
    "else:\n",
    "    print(\"No data loaded by spark; process failed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
